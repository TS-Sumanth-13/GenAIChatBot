{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPyLmKojGGwP"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install langchain-community\n",
        "!pip install openai\n",
        "!pip install gradio\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import LLMChain, PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "5I9d5RfntrD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY=\"insert your OpenAI API key here\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "9D85Vo3tt0BC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"You are a sports-loving high school student with a keen interest in multiple sports, from soccer and basketball to tennis and swimming. You closely follow sports events, stats, and news, making you the go-to person for all sports-related discussions and predictions.\n",
        "{chat_history}\n",
        "User: {user_message}\n",
        "Chatbot:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"user_message\"], template=template\n",
        ")\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
      ],
      "metadata": {
        "id": "z6MxeT7Ut1H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.llms import HuggingFacePipeline\n",
        "# hf = HuggingFacePipeline.from_model_id(\n",
        "#     model_id=\"gpt2\",\n",
        "#     task=\"text-generation\",)"
      ],
      "metadata": {
        "id": "VlLuBZK5uMES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(\n",
        "    llm=ChatOpenAI(temperature='0.5', model_name=\"gpt-3.5-turbo\"),\n",
        "    prompt=prompt,\n",
        "    verbose=True,\n",
        "    memory=memory,\n",
        ")"
      ],
      "metadata": {
        "id": "2Vcn2z9euQKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_response(user_message,history):\n",
        "    response = llm_chain.predict(user_message = user_message)\n",
        "    return response"
      ],
      "metadata": {
        "id": "rbjx8v4PuTRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo = gr.ChatInterface(get_text_response, examples=[\"How are you doing?\",\"What are your interests?\",\"Which places do you like to visit?\"])"
      ],
      "metadata": {
        "id": "l7Sv7uOSuWs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    demo.launch() #To create a public link, set `share=True` in `launch()`. To enable errors and logs, set `debug=True` in `launch()`."
      ],
      "metadata": {
        "id": "BdOBwKR7uaCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Publishing code to Hugging Face**"
      ],
      "metadata": {
        "id": "cPugVX-Hwpdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import gradio as gr\n",
        "\n",
        "PERPLEXITY_API_KEY = \"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\"  # your Perplexity key\n",
        "\n",
        "def call_perplexity_api(user_message, chat_history):\n",
        "    api_url = \"https://api.perplexity.ai/chat\"  # hypothetical endpoint, replace with real one\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {PERPLEXITY_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "    # Construct the payload based on Perplexity API specs\n",
        "    payload = {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a sports-loving high school student...\"},\n",
        "            # Add previous messages to simulate chat history, if supported\n",
        "            {\"role\": \"user\", \"content\": user_message},\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    response = requests.post(api_url, json=payload, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        # Extract the reply from Perplexity's response format\n",
        "        answer = data.get(\"answer\") or data.get(\"response\") or data.get(\"content\") or \"\"\n",
        "        return answer\n",
        "    else:\n",
        "        return f\"Error: {response.status_code} - {response.text}\"\n",
        "\n",
        "def chat(user_message, history):\n",
        "    # Here you can process history if needed (append, shorten, etc.)\n",
        "    # For now, just pass user_message forward\n",
        "    return call_perplexity_api(user_message, history)\n",
        "\n",
        "demo = gr.ChatInterface(chat)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(share=True)\n"
      ],
      "metadata": {
        "id": "GQRjIHIRwuJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "tiJauRP6w88O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "api = HfApi()"
      ],
      "metadata": {
        "id": "Iq0Zikywx7Vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HUGGING_FACE_REPO_ID = \"Sumanth-13/GenAIChatBot\""
      ],
      "metadata": {
        "id": "porHQHFmyB4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir /content/ChatBotWithOpenAI\n",
        "!wget -P  /content/ChatBotWithOpenAI/ https://s3.ap-south-1.amazonaws.com/cdn1.ccbp.in/GenAI-Workshop/ChatBotWithOpenAIAndLangChain/app.py\n",
        "!wget -P /content/ChatBotWithOpenAI/ https://s3.ap-south-1.amazonaws.com/cdn1.ccbp.in/GenAI-Workshop/ChatBotWithOpenAIAndLangChain/requirements.txt"
      ],
      "metadata": {
        "id": "d9LOTbYubojs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ChatBotWithOpenAI\n",
        "\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"./requirements.txt\",\n",
        "    path_in_repo=\"requirements.txt\",\n",
        "    repo_id=HUGGING_FACE_REPO_ID,\n",
        "    repo_type=\"space\")\n",
        "\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"./app.py\",\n",
        "    path_in_repo=\"app.py\",\n",
        "    repo_id=HUGGING_FACE_REPO_ID,\n",
        "    repo_type=\"space\")"
      ],
      "metadata": {
        "id": "hWPl2PgoztY_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}